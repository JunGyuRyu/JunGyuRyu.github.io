# Kafka & Airflow

[Apache Kafka](https://kafka.apache.org/)

# LinkdIn ê°œë°œ

<aside>
ğŸ’¡ ì¹´í”„ì¹´ëŠ” ë°ì´í„° íŒŒì´í”„ë¼ì¸(Data Pipeline)ì„ êµ¬ì¶•í•  ë•Œ ê°€ì¥ ë§ì´ ê³ ë ¤ë˜ëŠ” ì‹œìŠ¤í…œ ì¤‘ í•˜ë‚˜ë¡œ ë§í¬ë“œì¸ì—ì„œ ì²˜ìŒ ê°œë°œëœ ë¶„ì‚° ë©”ì‹œì§• í”Œë«í¼ì´ë©° ì˜¤í”ˆì†ŒìŠ¤ì…ë‹ˆë‹¤.

ëŠì„ì—†ì´ ë“¤ì–´ì˜¤ëŠ” ë°ì´í„°ë¥¼ ì¼ê´„ì ìœ¼ë¡œ ë¬¶ì–´ì„œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  ì‹¤ì‹œê°„ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê±°ë‚˜ ê°€ê³µí•˜ì—¬ ë˜ ë‹¤ë¥¸ ì„œë¹„ìŠ¤ì— ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ì—¬ IoT ë°ì´í„° ì²˜ë¦¬, ê¸ˆìœµê±°ë˜ ì‚¬ê¸°ë°©ì§€ ë“±ì— ì‚¬ìš©ë©ë‹ˆë‹¤. 

ë¿ë§Œ ì•„ë‹ˆë¼ ì˜¤í”ˆë§ˆì¼“ì—ì„œ ì‚¬ìš©ìì˜ í™ˆí˜ì´ì§€ í´ë¦­ ìˆ˜, ìƒí’ˆì„ ë°”ë¼ë³´ëŠ” ë¹ˆë„ ë° ì‹œê°„, ì£¼ë¬¸ ë° í™˜ë¶ˆ ì„œë¹„ìŠ¤ ë“± ì–´í”Œë¦¬ì¼€ì´ì…˜ ì‚¬ìš©ìì˜ í™œë™ ë¶„ì„ì„ í†µí•´ ê¸°ì—…ì—ì„œ ì „ëµì ìœ¼ë¡œ ë¹„ì¦ˆë‹ˆìŠ¤ë¥¼ í•  ìˆ˜ ìˆë„ë¡ ë°ì´í„°ë¥¼ ìˆ˜ì§‘/ë¶„ì„í•˜ëŠ” ë°ë„ ì‚¬ìš©ë©ë‹ˆë‹¤.

í˜„ì¬ëŠ” ë§í¬ë“œì¸ì—ì„œ ì¹´í”„ì¹´ë¥¼ ê°œë°œí•˜ë˜ ì¼ë¶€ ì—”ì§€ë‹ˆì–´ë“¤ì´ '[Confluent](https://www.confluent.io/)'ë¼ëŠ” íšŒì‚¬ë¥¼ ì„¤ë¦½í•˜ì—¬ ì¹´í”„ì¹´ì™€ ê´€ë ¨ëœ ì¼ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤.

</aside>

- íŠ¸ìœ„í„° : ì‚¬ìš©ìì—ê²Œ íŠ¸ìœ—ì„ ë³´ë‚´ê³  ë°›ì„ ë•Œ ì‚¬ìš©
- ë§í¬ë“œì¸: ì´ìš©ìì˜ ì‚¬ì´íŠ¸ í™œë™ ë°ì´í„°ë¥¼ ë¶„ì„ ë° ìš´ì˜í•˜ëŠ” ë° ì‚¬ìš©
- ë„·í”Œë¦­ìŠ¤: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ì´ë²¤íŠ¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì‚¬ìš©
- ëª¨ì§ˆë¼ íŒŒì´ì–´í­ìŠ¤: ìµœì¢… ì‚¬ìš©ì ë¸Œë¼ìš°ì €ì—ì„œ ì„±ëŠ¥ ë° ì‚¬ìš© ë°ì´í„°ë¥¼ ìˆ˜ì§‘

---

# 2024-03-20

# ì¹´í”„ì¹´ ë„ì»¤ë¡œ ì‹¤í–‰

VSCodeì—ì„œ yml íŒŒì¼ ì‘ì„±

- **kafka-compose.yml**
    
    ```yaml
    version: '1'
    services:
      zookeeper:
        image: wurstmeister/zookeeper:latest
        container_name: zookeeper
        ports:
          - "2181:2181"
      kafka:
        image: wurstmeister/kafka:latest
        container_name: kafka
        ports:
          - "9092:9092"
        environment:
          KAFKA_ADVERTISED_HOST_NAME: 127.0.0.1
          KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
        volumes:
          - /var/run/docker.sock:/var/run/docker.sock
    ```
    

- **í„°ë¯¸ë„ì—ì„œ ì•„ë˜ ì½”ë“œ ì‹¤í–‰**
    
    ```bash
    $ docker compose -f kafka-compose.yml up
    $ docker exec -it kafka /bin/bash
    $ cd /opt/kafka
    
    # Topic ìƒì„±
    ./bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --topic fisa-kafka-test
    
    # Topic ìƒì„± - partitions, replication-factor
    ./bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --topic fisa-kafka-test --partitions 1 --replication-factor 1
    
    # Topic ì„¤ì • ë³€ê²½
    # í™•ì¥ì€ ê°€ëŠ¥í•˜ì§€ë§Œ ì¶•ì†ŒëŠ” ë¶ˆê°€ëŠ¥
    ./bin/kafka-topics.sh --bootstrap-server localhost:9092 --topic fisa-kafka-test --alter --partitions 10
    
    # Topic í™•ì¸
    ./bin/kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic fisa-kafka-test
    
    # Topic ëª©ë¡
    ./bin/kafka-topics.sh --list --bootstrap-server localhost:9092
    
    # Topic ì‚­ì œ
    ./bin/kafka-topics.sh --delete --bootstrap-server localhost:9092 --topic fisa-kafka-test
    
    # Producer
    ./bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic fisa-kafka-test
    
    # Producer - Key, Value
    ./bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic fisa-kafka-test --property "parse.key=true" --property "key.separator=:" --property "print.key=true"
    ./bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic test-topic --property "parse.key=true" --property "key.separator=:" --property "print.key=true"
    
    # Producer - Message
    ABCDE
    # Producer - Key, Value Message
    key:{"val1":"A","val2":"B","val3":3}
    key1:value1
    key2:value2
    key3:value3
    
    # Consumer
    ./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic fisa-kafka-test --from-beginning
    
    # Consumer Group í™•ì¸
    ./bin/kafka-consumer-groups.sh --list --bootstrap-server localhost:9092
    
    # Cousumer Group ìƒì„±
    ./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic --group test-group --from-beginning --max-messages 1
    
    # Consumer Group Topic í™•ì¸
    ./bin/kafka-consumer-groups.sh --describe --bootstrap-server localhost:9092 --topic fisa-kafka-test --group test-group
    
    # Topic ìƒì„±ì‹œ ì˜¤ë¥˜ê°€ ë‚˜ëŠ” ê²½ìš°
    ./bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --command-config client.properties --replication-factor 3 --partitions 1 --topic not-working-topic
    ```
    

---

# 2024-03-22

---

# 2024-03-25

# AirFlow

- Flask ê¸°ë°˜ì˜ Workflow Management platform, WorkflowëŠ” DAGë¡œ í‘œì‹œëœë‹¤.

![Untitled](airflow_image/Untitled.png)

<aside>
ğŸ’¡ ETL: Extract, Transform and Load

- Extract : ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•´ì„œ ë°ì´í„° ë¤í”„ë¡œë¶€í„° ë°ì´í„°ë¥¼ ë°›ì•„ì˜¤ëŠ” ì‘ì—…ë“¤
- Transform : ë°ì´í„°ì˜ í˜•íƒœ ë° í¬ë§·ì„ ë°”ê¾¸ëŠ” ê²ƒ, ê²½ìš°ì— ë”°ë¼ì„œ ì´ ê³¼ì • ìƒëµ ê°€ëŠ¥
- Load: ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì—ì„œ í…Œì´ë¸” í˜•íƒœë¡œ ì ì¬ í˜¹ì€ ë ˆì´í¬ì— ì ì¬
</aside>

<aside>
ğŸ’¡ ETL vs ELT

- ETL: ë°ì´í„°ë¥¼ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ì™¸ë¶€ì—ì„œ ë‚´ë¶€ë¡œ ê°€ì ¸ì˜¤ëŠ” í”„ë¡œì„¸ìŠ¤
- ELT: ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ë‚´ë¶€ ë°ì´í„°ë¥¼ ì¡°ì‘í•´ì„œ (ë³´í†µì€ ì¢€ë” ì¶”ìƒí™”
ë˜ê³  ìš”ì•½ëœ) ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë§Œë“œëŠ” í”„ë¡œì„¸ìŠ¤
â–ª ì´ ê²½ìš° ë°ì´í„° ë ˆì´í¬(DWë³´ë‹¤ Scalable)ë¥¼ ì“°ê¸°ë„ í•¨
</aside>

## Airflow ì„¤ì¹˜ ë° ì‹¤í–‰í•˜ê¸°

## ê°€. Ubuntu í™˜ê²½ì—ì„œ Dockerë¡œ ì„¤ì¹˜ ë° ì‹¤í–‰

[Running Airflow in Docker â€” Airflow Documentation](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html)

```bash
$ ubuntu
$ mkdir airflow
$ cd airflow
$ curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.8.3/docker-compose.yaml'
$ mkdir -p ./dags ./logs ./plugins ./config
$ echo AIRFLOW_UID=50000 > .env
$ docker compose up airflow-init
$ docker compose up
```

## ë‚˜. ìˆ˜ì—…ì—ì„œ ì‚¬ìš©í•  Airflow

https://github.com/YeonjiKim0316/airflow_test

## ë‹¤. EC2ì— airflow ì„¤ì¹˜ ë° ì„¤ì •

1. AIRFLOWëŠ” ìµœì†Œ 4gb ë©”ëª¨ë¦¬ì™€ 10GBì˜ ì—¬ìœ ê³µê°„ì„ í•„ìš”ë¡œ í•©ë‹ˆë‹¤. t2.large / 20GBë¡œ EC2ë¥¼ ìƒì„±í•˜ê³  dockerë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.
2. ì¸ë°”ìš´ë“œ ê·œì¹™ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤
    
    ![Untitled](airflow_image/Untitled%201.png)
    

## ì°¸ê³ : EC2ì— Docker ì„¤ì¹˜í•˜ê¸°

í˜„ì¬ ìš°ë¦¬ì˜ EC2 í™˜ê²½ì€ í…… ë¹„ì–´ìˆëŠ” ê²ƒì´ë‚˜ ë‹¤ë¦„ì—†ê¸° ë•Œë¬¸ì—,Â `sudo apt-get update`Â ë¡œ ì„¤ì¹˜ ê°€ëŠ¥í•œ íŒ¨í‚¤ì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¨ í›„, Docker ë“± í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.Â 

- UbuntuëŠ” apt, CentOS(AWS AMI)ëŠ” yum ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ìš´ì˜ì²´ì œì— ë§ëŠ” ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

### 1) ìš°ë¶„íˆ¬ ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸

```bash
$ sudo apt-get update
$ curl https://get.docker.com/ | sudo sh  # ë„ì»¤ ì„¤ì¹˜
```

- ìœ„ ëª…ë ¹ì–´ê°€ ì‘ë™í•˜ì§€ ì•Šì„ ê²½ìš°?
    
    ### 2) í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
    
    ```bash
    sudo apt-get install apt-transport-https ca-certificates curl gnupg-agent software-properties-common
    ```
    
    ### 3) Dockerì˜ ê³µì‹ GPGí‚¤ë¥¼ ì¶”ê°€
    
    ```bash
    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
    ```
    
    ### 4) Dockerì˜ ê³µì‹ apt ì €ì¥ì†Œë¥¼ ì¶”ê°€
    
    ```bash
    sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
    ```
    
    ### 5) Docker ì„¤ì¹˜
    
    ```bash
    sudo apt-get install docker-ce docker-ce-cli containerd.io
    ```
    

### 6) Docker ì„¤ì¹˜ í™•ì¸í•˜ê¸°

- ë„ì»¤ ì‹¤í–‰ìƒíƒœ í™•ì¸

```bash
sudo docker ps
```

- ë„ì»¤ ì‹¤í–‰

```
sudo docker compose up
```

### ì°¸ê³ : sudo ì—†ì´ dockerëª…ë ¹ì–´ ì‚¬ìš©í•˜ê¸°

**1. í˜„ì¬ ì‚¬ìš©ìë¥¼ docker groupì— í¬í•¨**

> sudo usermod -aG docker ${USER}
> 

**2. í„°ë¯¸ë„ ì¬ì‹œì‘ í›„ ê²°ê³¼ í™•ì¸(ë„ì»¤ê°€ ë“±ë¡ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸)**

> id -nG
> 

- ì‹¤ìŠµ: í„°ë¯¸ë„ì—ì„œ ê°„ë‹¨í•œ ë„ì»¤ ë‚´ë¶€ ì½”ë“œ ë³€ê²½í•˜ê¸°
    
    ```bash
    sudo docker exec -it {ì»¨í…Œì´ë„ˆì´ë¦„} /bin/bash
    
    apt-get update
    apt-get install vim
    vi íŒŒì¼ëª…
    ```
    

## python ì½”ë“œ ì˜ˆì‹œ

```python
from airflow import DAG
from airflow.decorators import task
from datetime import datetime

@task
def print_hello():
    print("hello!")
    return "hello!"

@task
def print_goodbye():
    print("goodbye!")
    return "goodbye!"

with DAG(
    dag_id = 'HelloWorld_v2',
    start_date = datetime(2023,8,15),
    catchup=False,
    tags=['example'],
    schedule = '0 2 * * *'
) as dag:

    # Assign the tasks to the DAG in order
    print_hello() >> print_goodbye()

```

## ë°ì´í„° íŒŒì´í”„ë¼ì¸ì˜ ì…ë ¥ê³¼ ì¶œë ¥ì„ ëª…í™•íˆ ë¬¸ì„œí™”

- ë¹„ì§€ë‹ˆìŠ¤ ì˜¤ë„ˆë¥¼ ëª…ì‹œ : ëˆ„ê°€ ì´ ë°ì´í„°ë¥¼ ìš”ì²­í–ˆëŠ”ì§€ë¥¼ ê¸°ë¡ìœ¼ë¡œ ë‚¨ê²¨ì•¼ í¸í•´ì§‘ë‹ˆë‹¤.

<aside>
ğŸ’¡ **ë°ì´í„° ë¦¬ë‹ˆì§€**(ë°ì´í„° ê³„ë³´)
í˜„ì¬ ì“°ì´ëŠ” ë°ì´í„°ê°€ ì–´ë–»ê²Œ ìƒì„±ëê³ , ì–´ë–¤ ê³¼ì •ì„ ê±°ì³¤ìœ¼ë©°, ì–´ë””ì— ì“°ì´ê³  ìˆëŠ”ì§€ ë“±ì˜ ê³„ë³´ë¥¼ ê´€ë¦¬í•´ í˜„í™©ì„ íŒŒì•…í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. (ë°ì´í„° ì‚¬ìš©ì— ëŒ€í•œ ë¡œê·¸ ìˆ˜ì§‘)

</aside>

## **DM**(Data Mart), **DW**(Data Warehouse), **DL**(Data Lake)

[ë°ì´í„° ë ˆì´í¬, ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤, ë°ì´í„° ë§ˆíŠ¸ ë¹„êµ - í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ ì†”ë£¨ì…˜ ê°„ì˜ ì°¨ì´ì  - AWS](https://aws.amazon.com/ko/compare/the-difference-between-a-data-warehouse-data-lake-and-data-mart/)

- **DM**: í™œìš©í•  í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ìŒ“ì•„ë‘ëŠ” ê³µê°„
- **DW**: DMì— ê³µê¸‰í•  ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ê³³ì—ì„œ ìˆ˜ì§‘í•˜ê³ , ETLì„ ì‚¬ìš©í•´ ì£¼ì œë³„ë¡œ í†µí•© ë° ì €ì¥í•˜ëŠ” ê³µê°„
- **DL:** ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ë‹¤ì–‘í•œ í˜•íƒœ ê·¸ëŒ€ë¡œ í•œ ê³³ì— ì €ì¥í•˜ëŠ” ê³µê°„
    - DWì—ëŠ” í•„ìš”í•œ ë°ì´í„°ë§Œ ìœ ì§€í•˜ê³  ì´ì „ ë°ì´í„°ë¥¼ DL(ë˜ëŠ” ì €ì¥ì†Œ)ë¡œ ì´ë™

![Untitled](airflow_image/Untitled%202.png)

# Airflow MySQL ì—°ê²°

- **Admin â†’ Connections**
    
    ![Untitled](airflow_image/Untitled%203.png)
    
- **Add a new record**
    
    ![Untitled](airflow_image/Untitled%204.png)
    
- **MySQL ì •ë³´ ì…ë ¥ í›„ ì €ì¥**
    
    ![Untitled](airflow_image/Untitled%205.png)
    
- **python ì½”ë“œ**
    
    ```python
    # aws í…Œì´ë¸” í™•ì¸
    from datetime import datetime, timedelta
    from email.policy import default
    from textwrap import dedent
    
    from airflow import DAG
    from airflow.providers.mysql.operators.mysql import MySqlOperator
    
    default_args = {
        'depends_on_past': False,
        'retires': 1,
        'retry_delay': timedelta(minutes=5)
    }
    
    sql_create_table = """
        CREATE TABLE `employees` (
            `employeeNumber` int(11) NOT NULL,
            `lastName` varchar(50) NOT NULL,
            `firstName` varchar(50) NOT NULL,
            `extension` varchar(10) NOT NULL,
            `email` varchar(100) NOT NULL,
            `officeCode` varchar(10) NOT NULL,
            `reportsTo` int(11) DEFAULT NULL,
            `jobTitle` varchar(50) NOT NULL,
        PRIMARY KEY (`employeeNumber`)
        );
    """
    
    sql_insert_data = """
        insert  into `employees`(`employeeNumber`,`lastName`,`firstName`,`extension`,`email`,`officeCode`,`reportsTo`,`jobTitle`) values 
            (1002,'Murphy','Diane','x5800','dmurphy@classicmodelcars.com','1',NULL,'President'),
            (1056,'Patterson','Mary','x4611','mpatterso@classicmodelcars.com','1',1002,'VP Sales'),
            (1076,'Firrelli','Jeff','x9273','jfirrelli@classicmodelcars.com','1',1002,'VP Marketing'),
            (1088,'Patterson','William','x4871','wpatterson@classicmodelcars.com','6',1056,'Sales Manager (APAC)'),
            (1102,'Bondur','Gerard','x5408','gbondur@classicmodelcars.com','4',1056,'Sale Manager (EMEA)'),
            (1143,'Bow','Anthony','x5428','abow@classicmodelcars.com','1',1056,'Sales Manager (NA)'),
            (1165,'Jennings','Leslie','x3291','ljennings@classicmodelcars.com','1',1143,'Sales Rep'),
            (1166,'Thompson','Leslie','x4065','lthompson@classicmodelcars.com','1',1143,'Sales Rep'),
            (1188,'Firrelli','Julie','x2173','jfirrelli@classicmodelcars.com','2',1143,'Sales Rep'),
            (1216,'Patterson','Steve','x4334','spatterson@classicmodelcars.com','2',1143,'Sales Rep'),
            (1286,'Tseng','Foon Yue','x2248','ftseng@classicmodelcars.com','3',1143,'Sales Rep'),
            (1323,'Vanauf','George','x4102','gvanauf@classicmodelcars.com','3',1143,'Sales Rep'),
            (1337,'Bondur','Loui','x6493','lbondur@classicmodelcars.com','4',1102,'Sales Rep'),
            (1370,'Hernandez','Gerard','x2028','ghernande@classicmodelcars.com','4',1102,'Sales Rep'),
            (1401,'Castillo','Pamela','x2759','pcastillo@classicmodelcars.com','4',1102,'Sales Rep'),
            (1501,'Bott','Larry','x2311','lbott@classicmodelcars.com','7',1102,'Sales Rep'),
            (1504,'Jones','Barry','x102','bjones@classicmodelcars.com','7',1102,'Sales Rep'),
            (1611,'Fixter','Andy','x101','afixter@classicmodelcars.com','6',1088,'Sales Rep'),
            (1612,'Marsh','Peter','x102','pmarsh@classicmodelcars.com','6',1088,'Sales Rep'),
            (1619,'King','Tom','x103','tking@classicmodelcars.com','6',1088,'Sales Rep'),
            (1621,'Nishi','Mami','x101','mnishi@classicmodelcars.com','5',1056,'Sales Rep'),
            (1625,'Kato','Yoshimi','x102','ykato@classicmodelcars.com','5',1621,'Sales Rep'),
            (1702,'Gerard','Martin','x2312','mgerard@classicmodelcars.com','4',1102,'Sales Rep');
    """
    
    with DAG(
        'MySQL_operator',
        default_args = default_args,
        description = """
            1) 'employees' í…Œì´ë¸”ì„ DBì— ìƒì„±
            2) 'employees' í…Œì´ë¸”ì— ê°’ ë„£ê¸°
        """,
        schedule_interval = '@daily',
        start_date = datetime(2022, 1, 1),
        catchup = False,
        tags = ['mysql', 'AWS', 'test', 'employees']
    ) as dag:
        t1 = MySqlOperator(
            task_id="create_employees_table",
            mysql_conn_id="AWS_RDB",
            sql=sql_create_table,
        )
    
        t2 = MySqlOperator(
            task_id="insert_employees_data",
            mysql_conn_id="AWS_RDB",  # Connectionsì— ë“±ë¡í•œ ì´ë¦„ì„ ì‚¬ìš© 
            sql=sql_insert_data
        )
    
        t1 >> t2
    ```
    

---

# 2024-03-26

## AIRFLOWëŠ” ìµœì†Œ 4gb ë©”ëª¨ë¦¬ì™€ 10GBì˜ ì—¬ìœ ê³µê°„ì´ í•„ìš”

## Airflow API Json í˜•ì‹ìœ¼ë¡œ ë³´ê¸°

http://localhost:8080/api/v1/dags/task-relationship

http://localhost:8080/api/v1/dags/task-relationship/tasks
